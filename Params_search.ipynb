{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from Stupid_digits import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAFdCAYAAABcsVc4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEImoEEIIUYhEVAghhCjkf0FSPo089zjDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a7734c8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X are mormalizied digits to have overall activity 10\n",
    "X,y = stupid_digits_dataset(100)\n",
    "plt.figure('data')\n",
    "for i in np.arange(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(X[i].reshape((5,5)), interpolation=None)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "#import brian2cuda\n",
    "#set_device(\"cuda_standalone\")\n",
    "#import brian2genn\n",
    "#set_device('genn')\n",
    "import numpy as np\n",
    "from time import clock\n",
    "\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "def sv_json(obj, p,encoding=\"cp1251\"):\n",
    "    with codecs.open(p, \"w\",encoding=encoding) as f:\n",
    "        json.dump(obj,f,indent=1,ensure_ascii=0)\n",
    "        \n",
    "def ld_json(p,encoding=\"cp1251\"):\n",
    "    with codecs.open(p,\"r\",encoding=encoding) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "start_scope()\n",
    "\n",
    "time_per_image = 100*ms\n",
    "time_step = 0.1*ms\n",
    "tau = 10*ms # tau for neuron's voltage 'v'\n",
    "tau_I = 15*ms # tau for neuron's current 'I'\n",
    "tau_h = 50*ms # tau for neuron's treshold\n",
    "\n",
    "wmax = 1.\n",
    "decay = 0.001\n",
    "\n",
    "eqs_input_neuron = '''\n",
    "rates : Hz\n",
    "da/dt = -a/alpha : 1\n",
    "dtheta/dt = -theta/beta : 1\n",
    "diff = a - c_diff * theta : 1\n",
    "train :1\n",
    "'''\n",
    "\n",
    "eqs_output_neuron = '''\n",
    "dv/dt = (-v+I)/tau : 1 (unless refractory)\n",
    "dI_inp/dt = -I_inp/tau_I :1\n",
    "dI_intr/dt = -I_intr/tau_I :1\n",
    "I_teacher :1\n",
    "I = I_inp + I_intr + I_teacher : 1\n",
    "da/dt = -a/alpha : 1\n",
    "dtheta/dt = -theta/beta : 1\n",
    "diff = a - c_diff * theta : 1\n",
    "dhold_output/dt = -hold_output/tau_h  : 1\n",
    "fixed_hold_output = clip(hold_output, 1, 5) :1\n",
    "train :1\n",
    "'''\n",
    "#fixed_hold_output = clip(hold_output, 1, 100) :1\n",
    "\n",
    "eqs_input_syn = '''\n",
    "w : 1\n",
    "'''\n",
    "\n",
    "# equations that describe changes if presynaptic spike of the forward-riented synapse of input layer occures\n",
    "eqs_input_pre = '''\n",
    "I_inp_post += w * c_inp\n",
    "a_pre += 1./classes *1*ms/(alpha)\n",
    "theta_pre += 1./classes *1*ms/(beta)\n",
    "'''\n",
    "\n",
    "# equations that describe changes if postsynaptic spike of the forward synapse occures\n",
    "eqs_input_post = '''\n",
    "a_post += 1./n_input *1*ms/(alpha)\n",
    "theta_post += 1./n_input *1*ms/(beta)\n",
    "w = clip(w + train_post*(-decay + lr*diff_pre), 0, wmax)\n",
    "'''\n",
    "\n",
    "eqs_intrinsic_output_syn = '''\n",
    "w: 1\n",
    "'''\n",
    "\n",
    "# equations that describe changes if spike of the intr output synapse occures\n",
    "eqs_intrinsic_output_pre = '''\n",
    "I_intr_post += w * c_intr_out\n",
    "'''\n",
    "\n",
    "eqs_intrinsic_output_post = '''\n",
    "w = clip(w + train_pre*(-lr_intr*diff_pre), -wmax, 0)\n",
    "'''\n",
    "\n",
    "\n",
    "reset_output = '''\n",
    "v = 0\n",
    "hold_output += 0.1*classes\n",
    "'''\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, X, y, params_to_optimize, inits = None, monitor = None, cheat = False, mod = True, \n",
    "                high_verbosity=True):\n",
    "        \n",
    "        '''\n",
    "        X - np.array with shape (N samples, N features)\n",
    "        y - np.array with shape (N samples)\n",
    "        n_hiiden - number of hidden neurons\n",
    "        inits  - list of 2 lists that contain 'distribution' and 'condition' parameters \n",
    "        for input, intrinsic, output and intrinsic_output connections\n",
    "            Example: inits = [['equal_[0,1]', None], ['equal_[-1,0]', 'i!=j']]\n",
    "        monitor - dictionary with keys 'P', 'G', 'H' or connection names. Each key argument value is dictionary.\n",
    "        Contains a list of what parameters to record for the object in the key name and dt.\n",
    "            Example: monitor = {'H':{'variables':[['a','I'],'dt':25*ms, 'record':True]}}\n",
    "        cheat - True if you wanna use pretrained model with first layer initialized from some objects of training data\n",
    "        '''\n",
    "        \n",
    "        self.high_verbosity = high_verbosity\n",
    "        global alpha, beta, lr, lr_intr, c_inp, c_intr_out, c_diff, Teacher_amplitude\n",
    "\n",
    "        alpha = params_to_optimize['alpha']\n",
    "        beta = params_to_optimize['beta']\n",
    "        lr = params_to_optimize['lr']\n",
    "        lr_intr = params_to_optimize['lr_intr']\n",
    "        c_inp = params_to_optimize['c_inp']\n",
    "        c_intr_out = params_to_optimize['c_intr_out']\n",
    "        c_diff = params_to_optimize['c_diff']\n",
    "        Teacher_amplitude = params_to_optimize['Teacher_amplitude']\n",
    "        \n",
    "        self.X = X\n",
    "        if self.X is not None:\n",
    "            n_input = int(X.shape[1])\n",
    "        else:\n",
    "            raise ValueError('No data provided to the simulation')\n",
    "        \n",
    "        self.y = y\n",
    "        classes = int(len(set(y)))\n",
    "        \n",
    "        # list to store initial values for synapses in order [input, intrinsic, output, intrinsic_output]\n",
    "        if inits:\n",
    "            pass\n",
    "        else:\n",
    "            inits = [['equal_[0,1]', None], ['equal_[-1,0]', 'i!=j']]\n",
    "         \n",
    "        #creatitng the network: groups and synapses\n",
    "        \n",
    "        self.P = NeuronGroup(n_input, eqs_input_neuron, threshold='rand()<rates*dt', method='linear', \n",
    "                             refractory=2*ms, dt = time_step, name='P')\n",
    "        #experiment\n",
    "        self.M = NeuronGroup(n_input, eqs_input_neuron, threshold='rand()<rates*dt', method='linear',\n",
    "                             refractory=2*ms, dt = time_step, name='M')\n",
    "        self.H = NeuronGroup(classes, eqs_output_neuron, method=linear, threshold='v > fixed_hold_output', \n",
    "                             reset=reset_output, refractory=2*ms, dt=time_step, name = 'H')\n",
    "        self.create_synapse('input_syn', self.P, self.H, eqs_input_syn, eqs_input_pre, eqs_input_post,\n",
    "                           distribution=inits[0][0], condition=inits[0][1])\n",
    "        self.create_synapse('input_inverse_syn', self.M, self.H, eqs_input_syn, eqs_input_pre, eqs_input_post,\n",
    "                           distribution=inits[0][0], condition=inits[0][1])\n",
    "        self.create_synapse('intrinsic_output_syn', self.H, self.H, eqs_intrinsic_output_syn, \n",
    "                            eqs_intrinsic_output_pre, eqs_intrinsic_output_post, \n",
    "                            distribution=inits[1][0], condition=inits[1][1])\n",
    "        \n",
    "        # set random values to a and theta for all groups, set train parameter of the group to 1.\n",
    "        self.random_init_groups([self.P, self.M, self.H], [1, 1, 1])\n",
    "        \n",
    "        # operation to do during simulation every dt \n",
    "        self.network_op = NetworkOperation(self.update_func, dt=time_per_image)\n",
    "        \n",
    "        # params to give for constructing the simulation\n",
    "        params = [self.P, self.M, self.H, self.input_syn, self.input_inverse_syn, \n",
    "                  self.intrinsic_output_syn, self.network_op]\n",
    "        \n",
    "        self.monitor = monitor\n",
    "        # adding monitors to the simulation params\n",
    "        # TO DO: OPTIMIZE!!!!!\n",
    "        if self.monitor:\n",
    "            if 'P' in self.monitor.keys():\n",
    "                self.StateMonitorP = StateMonitor(self.P, self.monitor['P']['variables'], record=self.monitor['P']['record'],\n",
    "                                                  dt=monitor['P']['dt'],\n",
    "                                                  name = 'StateMonitorP')\n",
    "                params.append(self.StateMonitorP)\n",
    "            else:\n",
    "                self.StateMonitorP = None\n",
    "                \n",
    "            if 'M' in monitor.keys():\n",
    "                self.StateMonitorM = StateMonitor(self.M, self.monitor['M']['variables'], record=self.monitor['M']['record'], \n",
    "                                                  dt=monitor['M']['dt'],\n",
    "                                                  name = 'StateMonitorM')\n",
    "                params.append(self.StateMonitorM)\n",
    "            else:\n",
    "                self.StateMonitorM = None\n",
    "                \n",
    "            if 'H' in monitor.keys():\n",
    "                self.StateMonitorH = StateMonitor(self.H, self.monitor['H']['variables'], record=self.monitor['H']['record'], \n",
    "                                                  dt=monitor['H']['dt'],\n",
    "                                                  name = 'StateMonitorH')\n",
    "                params.append(self.StateMonitorH)\n",
    "            else:\n",
    "                self.StateMonitorH = None\n",
    "                \n",
    "            if 'Input_weights' in monitor.keys():\n",
    "                self.SynapseMonitorInput = StateMonitor(self.input_syn, self.monitor['Input_weights']['variables'], \n",
    "                                                        record=self.monitor['Input_weights']['record'],\n",
    "                                                        dt = monitor['Input_weights']['dt'],\n",
    "                                                        name = 'SynapseMonitorInput')\n",
    "                params.append(self.SynapseMonitorInput)\n",
    "            else:\n",
    "                self.SynapseMonitorInput = None\n",
    "                \n",
    "            if 'Input_inverse_weights' in monitor.keys():\n",
    "                self.SynapseMonitorInputInverse = StateMonitor(self.input_inverse_syn, self.monitor['Input_inverse_weights']['variables'], \n",
    "                                                        record=self.monitor['Input_inverse_weights']['record'],\n",
    "                                                        dt = monitor['Input_inverse_weights']['dt'],\n",
    "                                                        name = 'SynapseMonitorInputInverse')\n",
    "                params.append(self.SynapseMonitorInputInverse)\n",
    "            else:\n",
    "                self.SynapseMonitorInputInverse = None\n",
    "            \n",
    "            if 'Intrinsic_output_weights' in monitor.keys():\n",
    "                self.SynapseMonitorIntrinsicOutput = StateMonitor(self.intrinsic_output_syn, self.monitor['Intrinsic_output_weights']['variables'],\n",
    "                                                        record=self.monitor['Intrinsic_output_weights']['record'], \n",
    "                                                        dt = monitor['Intrinsic_output_weights']['dt'],\n",
    "                                                        name = 'SynapseMonitorIntrinsicOutput')\n",
    "                params.append(self.SynapseMonitorIntrinsicOutput)\n",
    "            else:\n",
    "                self.SynapseMonitorIntrinsicOutput = None\n",
    "        \n",
    "        # setting params to the simulation\n",
    "        self.network = Network(params)\n",
    "        if self.high_verbosity:\n",
    "            print ('Network created using:')\n",
    "            for param in params:\n",
    "                print ('\\t'+param.name + '   dt = '+str(param.clock.dt))\n",
    "            \n",
    "        self.shown_labels = [] #list to store shown labels during lifetime of the simulation\n",
    "        self.predictions = [] #list to store preds during lifetime of the simulation\n",
    "        self.counter = 0 # counter for shown images during lifetime of the simulation\n",
    "        self.mod = mod # binary, True if train mod, False for test mod\n",
    "        self.clocks = [] # clocks list per image to monitor if there are to many params to record and RAM is dying \n",
    "        self.clocks.append(0)\n",
    "        \n",
    "        # never use this\n",
    "        if cheat:\n",
    "            dig_y = y[:int(n_hidden)]\n",
    "            dig_X = X[:int(n_hidden)]\n",
    "            for s in np.arange(int(n_hidden)):\n",
    "                for j in np.arange(int(n_input)):\n",
    "                    # shape dig_X = (n_hidden, n_input)\n",
    "                    # shape inp_syn is a vectorized matrix\n",
    "                    # W_11 ..... W_n1\n",
    "                    # ..\n",
    "                    # W_1m ..... W_nm\n",
    "                    # where n is post length, m is pre length\n",
    "                    self.input_syn.w[s+j*int(n_hidden)] = dig_X[s][j]\n",
    "            print ('ALARRRM !!! CHEATER !!! I used cheat initialisation')\n",
    "    \n",
    "    def random_init_groups(self, groups, trains):\n",
    "        '''\n",
    "        random inits for a and theta of neuron groups + setting trains for neuron groups\n",
    "        groups - list of NeuronGroup objects, trains - list of train values (Ex. [1, 1, 1] if all are trainable)'''\n",
    "        if self.high_verbosity:\n",
    "            print ()\n",
    "        for group,tr in zip(groups, trains):\n",
    "            if tr:\n",
    "                group.train = np.ones_like(group.train)\n",
    "                if self.high_verbosity:\n",
    "                    print (group.name + ' ' + str(group.N) + ' trainable')\n",
    "            else:\n",
    "                group.train = np.zeros_like(group.train)\n",
    "                if self.high_verbosity:\n",
    "                    print (group.name + ' non-trainable')\n",
    "            group.a = np.random.rand(len(group))*0.5\n",
    "            group.theta = np.random.rand(len(group))*0.5\n",
    "        if self.high_verbosity:\n",
    "            print ()\n",
    "    \n",
    "    def create_synapse(self, name, pre, post, eqs, on_pre, on_post, distribution = 'equal_[0,1]', condition = None):\n",
    "        \n",
    "        '''\n",
    "        AUTO-creating synapses between pre and post NeuronGroup with name='name'\n",
    "        eqs, on_pre, on_post -  standart parameteres for Synapses\n",
    "        distribution - string, initial values for weights\n",
    "        condition - string, some cpecific conditions for connecting, same as in connect() method for Synapses\n",
    "        extra value for conditions is 'reciprocal'. Should be used if Synapse object has w_rec reciprocal weights'''\n",
    "        \n",
    "        exec('self.'+ name + '= Synapses(pre, post, eqs, on_pre = on_pre, on_post = on_post, dt = time_step)')\n",
    "        if self.high_verbosity:\n",
    "            print ('{} -> {}, initial distribution = {}'.format(pre.name, post.name, distribution))\n",
    "        if condition:\n",
    "            if condition == 'reciprocal':\n",
    "                exec('self.'+ name + '.connect()')\n",
    "            else:\n",
    "                exec('self.'+ name + \".connect(condition = '\" + condition +\"')\")\n",
    "        else: \n",
    "            exec('self.'+ name + '.connect()')\n",
    "        \n",
    "        if distribution:\n",
    "            flag = 0\n",
    "        if distribution == 'equal_[0,1]':    \n",
    "            exec('self.' + name +'.w = np.random.random(len(pre)*len(post))')\n",
    "            flag = 1\n",
    "        if distribution == 'equal_[-1,1]'and condition == 'i!=j':\n",
    "            exec('self.' + name +'.w = (np.random.random(len(pre)*(len(post)-1)) - 0.5) * 2.')\n",
    "            flag = 1\n",
    "        if distribution == 'equal_[-1,0]'and condition == 'i!=j':\n",
    "            exec('self.' + name +'.w = (np.random.random(len(pre)*(len(post)-1)) - 1.)')  \n",
    "            flag = 1\n",
    "        if distribution == 'norm':\n",
    "            exec('self.' + name +'.w = np.random.randn(len(pre)*len(post))')\n",
    "            flag = 1\n",
    "        if distribution == 'zeros':\n",
    "            exec('self.' + name +'.w = np.zeros_like(self.' + name +'.w)')     \n",
    "            flag = 1\n",
    "        if distribution == 'ones':\n",
    "            exec('self.' + name +'.w = np.ones_like(self.' + name +'.w)')\n",
    "            flag = 1\n",
    "            \n",
    "        if condition == 'reciprocal':\n",
    "            exec('self.' + name +'.w_rec = np.array(self.' + name +'.w).transpose()')\n",
    "            flag = 1\n",
    "        \n",
    "        if self.high_verbosity and flag==0:\n",
    "            print ('SOMETHING WENT WRONG, check your distribution and condition paramaeters: {}, {}'.format(distribution, condition))\n",
    "        \n",
    "        \n",
    "    def sample_data(self):\n",
    "        '''\n",
    "        Sampling random X vector from X and correspondinf label from y\n",
    "        '''\n",
    "        sample = np.random.randint(0, self.X.shape[0])\n",
    "        self.clocks.append(clock())\n",
    "        return self.X[sample], self.y[sample]\n",
    "\n",
    "    def update_func(self):\n",
    "            \n",
    "        # active phase\n",
    "        if self.counter % 2 == 0:\n",
    "        \n",
    "            image_sample, label_sample = self.sample_data()\n",
    "            if self.high_verbosity:\n",
    "                print ('Processing {} image with label {}'.format(self.counter/2, label_sample))\n",
    "\n",
    "            self.P.rates = [k*250*Hz for k in image_sample]\n",
    "            self.M.rates = [(1-k)*250*Hz for k in image_sample]\n",
    "            if self.mod:\n",
    "                self.H.I_teacher = np.zeros_like(self.H.I_teacher)\n",
    "                self.H.I_teacher[int(label_sample)] = Teacher_amplitude\n",
    "                #self.H.train = np.zeros_like(self.H.train)\n",
    "                #self.H.train[int(label_sample)] = 1.\n",
    "            else: \n",
    "                self.H.I_teacher = np.zeros_like(self.H.I_teacher)\n",
    "                self.H.train = np.zeros_like(self.H.train)\n",
    "            # saving shown labels\n",
    "            self.shown_labels.append(int(label_sample))\n",
    "        \n",
    "        # resting phase\n",
    "        else:\n",
    "            self.P.rates = np.zeros_like(self.P.rates)\n",
    "            self.M.rates = np.zeros_like(self.M.rates)\n",
    "            self.H.I_inp = np.zeros_like(self.H.I_inp)\n",
    "            self.H.I_intr = np.zeros_like(self.H.I_intr)\n",
    "            if self.mod:\n",
    "                self.H.I_teacher = np.zeros_like(self.H.I_teacher)\n",
    "                #self.H.train = np.zeros_like(self.H.train)\n",
    "                \n",
    "            # saving predictions\n",
    "            pred = np.argmax(np.mean(self.StateMonitorH.a[:,-int(time_per_image/(2.*ms)):], axis=1))\n",
    "            self.predictions.append(pred)\n",
    "            if self.high_verbosity:\n",
    "                #print(self.StateMonitorH['a'][:,-1])\n",
    "                print(np.max(self.StateMonitorH.a[:,-1]))\n",
    "                print(np.max(self.StateMonitorH.I_inp[:,-1]))\n",
    "                print(np.min(self.StateMonitorH.I_intr[:,-1]))\n",
    "                if self.predictions[-1] == self.shown_labels[-1]:\n",
    "                    print ('correct', pred)\n",
    "                else:\n",
    "                    print ('incorrect', pred)\n",
    "        self.counter += 1\n",
    "\n",
    "    def run(self, runtime):\n",
    "        self.network.run(runtime)\n",
    "        \n",
    "    def save_weights(self, path, encoding=\"cp1251\"):\n",
    "        print (\"сохранение сети в файл\", path)\n",
    "        weights = {}\n",
    "        weights['Input_weights'] = list(self.input_syn.w)\n",
    "        weights['Intrinsic_output_weights'] = list(self.intrinsic_output_syn.w)\n",
    "        weights['Input_inverse_weights'] = list(self.input_inverse_syn.w)\n",
    "        \n",
    "        sv_json(weights, path, encoding=encoding)\n",
    "    \n",
    "    def load_weights(self, path, encoding=\"cp1251\"):\n",
    "        print (\"загрузка сети из файла\", path)\n",
    "        weights = ld_json(path, encoding=encoding)\n",
    "        self.input_syn.w = weights['Input_weights']\n",
    "        self.intrinsic_output_syn.w = weights['Intrinsic_output_weights']\n",
    "        self.input_inverse_syn.w = weights['Input_inverse_weights']\n",
    "        \n",
    "    def plot_clocks(self):\n",
    "        '''\n",
    "        Plot values of np.diff(self.clocks) to look of there are some errors during \"run\". If there \n",
    "        are huge picks in the middle of simulation on this graph, than it is likely that there are to \n",
    "        much recording variables or neurons in Monitors.\n",
    "        '''\n",
    "        figure('plot_clocks')\n",
    "        plot(np.arange(1, len(self.clocks)), np.diff(self.clocks))\n",
    "        if self.high_verbosity:\n",
    "            print('INIT', np.sum(np.diff(self.clocks))[:2])\n",
    "            print('MEAN', np.mean(np.diff(self.clocks)[2:]), 'STD', np.std(np.diff(self.clocks))[2:])\n",
    "        show()\n",
    "    \n",
    "    def plot_H(self, variables, interval, neuron_indexes):\n",
    "        '''\n",
    "        Plotting all recording variables for H group on 'interval' period for 'neuron_indexes' neurons.\n",
    "        Note that neuron_indexes is a sub-list from monitor['H'][2] list (remember if it was 'True', than all neurons \n",
    "        of this NeuronGroup were recorded)\n",
    "        '''\n",
    "        figure('H', figsize=(10, len(variables)*5))\n",
    "        for _, m in enumerate(variables):\n",
    "            subplot(len(variables), 1, _+1)\n",
    "            title(m)\n",
    "            for j in neuron_indexes:\n",
    "                exec('plot(interval, self.StateMonitorH.'+str(m)+'[j][interval[0]:interval[-1]+1], label = str(j))')\n",
    "            legend(loc='best')\n",
    "        show()\n",
    "        \n",
    "    def plot_weights(self, weights_type, interval, weights_indexes):\n",
    "        '''\n",
    "        Plotting weights evolution for 'weights_type' on 'interval' period of time for 'weights_indexes' weights.\n",
    "        Note that weights_indexes is a sub-list from monitor[$weights_type][2] list (remember if it was 'True', than all weights \n",
    "        of this type in this Synapse are recorded)\n",
    "        '''\n",
    "        \n",
    "        if weights_type == 'Input_weights':\n",
    "            if self.SynapseMonitorInput:\n",
    "                values = self.SynapseMonitorInput.w\n",
    "            else:\n",
    "                raise NameError('There was no record of this synapse')\n",
    "            \n",
    "        elif weights_type == 'Input_inverse_weights':\n",
    "            if self.SynapseMonitorInputInverse:\n",
    "                values = self.SynapseMonitorInputInverse.w\n",
    "            else:\n",
    "                raise NameError('There was no record of this synapse')\n",
    "        \n",
    "        elif weights_type == 'Intrinsic_output_weights':\n",
    "            if self.SynapseMonitorIntrinsicOutput:\n",
    "                values = self.SynapseMonitorIntrinsicOutput.w\n",
    "            else:\n",
    "                raise NameError('There was no record of this synapse')\n",
    "        else:\n",
    "            raise NameError('Incorrect weights_type')\n",
    "        \n",
    "        \n",
    "        figure(weights_type, figsize=(10, 5))\n",
    "        title(weights_type)\n",
    "        for j in weights_indexes:\n",
    "            plot(interval, values[j], label = str(j))\n",
    "        #legend(loc='best')\n",
    "        show()\n",
    "        \n",
    "    def imshow_forward_weights(self, gr, N, M, n, m):\n",
    "        '''\n",
    "        Plotting forward weights of 'gr' NeuronGroup on one figure.\n",
    "        gr - 'H'\n",
    "        N, M - height and width of figure in subplots\n",
    "        n, m - height and width of weight image\n",
    "        '''\n",
    "\n",
    "        if gr == 'H':\n",
    "            group1 = self.P\n",
    "            group2 = self.H\n",
    "            group3 = self.M\n",
    "            name1 = 'imshow_input_weights'\n",
    "            name2 = 'imshow_input_inverse_weights'\n",
    "            np_images1 = np.array(self.input_syn.w)\n",
    "            np_images2 = np.array(self.input_inverse_syn.w)\n",
    "        else:\n",
    "            raise NameError('No such group')\n",
    "\n",
    "        images_shape1=[len(group1), len(group2)]\n",
    "        self.one_image(np_mts=np_images1, mts_shape=images_shape1, N=N, M=M, n=n, m=m, frame=None,\n",
    "                       name=name1)\n",
    "        \n",
    "        images_shape2=[len(group3), len(group2)]\n",
    "        self.one_image(np_mts=np_images2, mts_shape=images_shape2, N=N, M=M, n=n, m=m, frame=None,\n",
    "                       name=name2)\n",
    "        \n",
    "    \n",
    "    def imshow_intrinsic_weights(self, gr, N, M, n, m):\n",
    "        '''\n",
    "        Plotting intrinsic weights of 'gr' NeuronGroup on one figure.\n",
    "        gr - 'H'\n",
    "        N, M - height and width of figure in subplots\n",
    "        n, m - height and width of weight image\n",
    "        '''\n",
    "        \n",
    "        if gr == 'H':\n",
    "            group = self.H\n",
    "            z = self.intrinsic_output_syn.w\n",
    "        else:\n",
    "            raise NameError('No such group')\n",
    "        \n",
    "        for i in np.arange(int(len(group))):\n",
    "            z = insert(z, i+int(len(group))*i, 0)\n",
    "    \n",
    "        np_mts = np.array(z)\n",
    "        mts_shape=[len(group), len(group)]\n",
    "        \n",
    "        self.one_image(np_mts=np_mts, mts_shape=mts_shape, N=N, M=M, n=n, m=m, intrinsic=True, \n",
    "                       name='imshow_intrinsic_weights')\n",
    "        \n",
    "    def one_image(self, np_mts, mts_shape, N, M, n, m, frame=None, intrinsic=False, name=None, video=False):\n",
    "        '''\n",
    "        Plotting image of np_mts frame\n",
    "        np_mts - numpy array of multivariate timeseries (or just numpy array for fixed frame)\n",
    "        mts_shape - list [amount of neurons in pre group, amount of neurons in post group]\n",
    "        frame - frame of timeseries, None if np_mts is just numpy array of weights instead of synapse monitor mts\n",
    "        N, M - height and width of figure in \"subplots\"\n",
    "        n, m - height and width of weight image\n",
    "        intrinsic - True if visualising intrinsic weights\n",
    "        name - unique name of the figure\n",
    "        '''\n",
    "        if intrinsic:\n",
    "            images = np.array(np_mts).reshape(int(mts_shape[1]), int(n), int(m))\n",
    "        else:\n",
    "            images = [[np_mts[r + j * int(mts_shape[1])][frame]\n",
    "                        for j in np.arange(mts_shape[0])] \n",
    "                        for r in np.arange(mts_shape[1])]\n",
    "            images = np.array(images).reshape(int(mts_shape[1]), int(n), int(m))\n",
    "\n",
    "        horizontal_lines = np.ones((1, M*(m+1)+1))*(np.nan)\n",
    "\n",
    "        for i in np.arange(N):\n",
    "            horizontal_line = np.ones((images[0].shape[0],1))*(np.nan)\n",
    "            for j in np.arange(M):\n",
    "                im = np.hstack((images[int(j+i*M)],np.ones((images[int(j+i*M)].shape[0],1))*(np.nan)))\n",
    "                horizontal_line = np.hstack((horizontal_line, im))\n",
    "            horizontal_line = np.vstack((horizontal_line, np.ones((1, M*(m+1)+1))*(np.nan)))\n",
    "            horizontal_lines = np.vstack((horizontal_lines, horizontal_line))\n",
    "        \n",
    "        if video:\n",
    "            return horizontal_lines\n",
    "        else:\n",
    "            figure(name, figsize=(4, 4*N/M))\n",
    "            title(name)\n",
    "            imshow(horizontal_lines, interpolation=None, cmap='winter')\n",
    "            colorbar()\n",
    "            axis('off')\n",
    "            show()\n",
    "\n",
    "    def animate_input_weights(self, name, N, M, n, m, start=0, end=None):\n",
    "        group1 = self.P\n",
    "        group2 = self.H\n",
    "        np_images = np.array(self.SynapseMonitorInput.w)\n",
    "        images_shape=[len(group1), len(group2)]\n",
    "        if end:\n",
    "            pass\n",
    "        else:\n",
    "            end = self.SynapseMonitorInput.w.shape[1]-1\n",
    "        \n",
    "        print (end)\n",
    "        NN.animate_weights(name, np_mts=np_images, mts_shape=images_shape, start=start, end=end,\n",
    "                           N=N, M=M, n=n, m=m, intrinsic=False)\n",
    "        \n",
    "    def animate_input_inverse_weights(self, name, N, M, n, m, start=0, end=None):\n",
    "        group1 = self.M\n",
    "        group2 = self.H\n",
    "        np_images = np.array(self.SynapseMonitorInputInverse.w)\n",
    "        images_shape=[len(group1), len(group2)]\n",
    "        if end:\n",
    "            pass\n",
    "        else:\n",
    "            end = self.SynapseMonitorInputInverse.w.shape[1]-1\n",
    "        \n",
    "        print (end)\n",
    "        NN.animate_weights(name, np_mts=np_images, mts_shape=images_shape, start=start, end=end,\n",
    "                           N=N, M=M, n=n, m=m, intrinsic=False)\n",
    "   \n",
    "    def animate_weights(self, name, np_mts, mts_shape, start, end, N, M, n, m, intrinsic=False):\n",
    "\n",
    "        def frames_generator(np_mts, mts_shape, start, end, N, M, n, m, intrinsic=False):\n",
    "            '''\n",
    "            generator for speed and RAM economy(hope it helps)\n",
    "            '''\n",
    "            frame = start\n",
    "            while frame < end:\n",
    "                image_frame = self.one_image(np_mts, mts_shape, N, M, n, m, \n",
    "                                             frame=frame, intrinsic=intrinsic, video=True)\n",
    "                frame += 1\n",
    "                yield image_frame\n",
    "        \n",
    "        def update(data):\n",
    "            mat.set_data(data)\n",
    "            return mat \n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        mat = ax.imshow(self.one_image(np_mts[:,0], mts_shape, N, M, n, m, \n",
    "                                       frame=None, intrinsic=intrinsic, video=True),cmap=\"winter\",interpolation=None)\n",
    "        plt.colorbar(mat)\n",
    "        plt.axis('off')\n",
    "        print ('im here')\n",
    "        ani = animation.FuncAnimation(fig, update, frames_generator(np_mts, mts_shape, start, end, N, M, n, m, intrinsic=False), \n",
    "                                      interval=10, save_count=end-2)\n",
    "        print ('saving file ', name +'.mp4')\n",
    "        ani.save(name+'.mp4',writer=animation.FFMpegFileWriter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    \"i\" is an internal variable of group \"synapses_2\", but also exists in the run namespace with the value 15. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SIMULATION', 0, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 10.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 25. * msecond, 'c_intr_out': 0.10000000000000001})\n",
      "train\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    \"i\" is an internal variable of group \"synapses_5\", but also exists in the run namespace with the value 15. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n",
      "('SIMULATION', 1, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 25.0, 'beta': 150. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 15. * msecond, 'c_intr_out': 0.5})\n",
      "train\n",
      "test\n",
      "0.7\n",
      "('SIMULATION', 2, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 60.0, 'beta': 110. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 30. * msecond, 'c_intr_out': 15.0})\n",
      "train\n",
      "test\n",
      "0.23\n",
      "('SIMULATION', 3, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 45.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 10. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    \"i\" is an internal variable of group \"synapses_8\", but also exists in the run namespace with the value 15. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SIMULATION', 4, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 18.75, 'beta': 150. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 20. * msecond, 'c_intr_out': 18.75})\n",
      "train\n",
      "test\n",
      "0.21\n",
      "('SIMULATION', 5, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 15.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 10. * msecond, 'c_intr_out': 3.0})\n",
      "train\n",
      "test\n",
      "0.36\n",
      "('SIMULATION', 6, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 20.0, 'beta': 80. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 15. * msecond, 'c_intr_out': 3.5})\n",
      "train\n",
      "test\n",
      "0.27\n",
      "('SIMULATION', 7, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 15.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 20. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.49\n",
      "('SIMULATION', 8, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 12.5, 'beta': 150. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 25. * msecond, 'c_intr_out': 25.0})\n",
      "train\n",
      "test\n",
      "0.32\n",
      "('SIMULATION', 9, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 75.0, 'beta': 150. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 25. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.4\n",
      "('SIMULATION', 10, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 150.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 30. * msecond, 'c_intr_out': 50.0})\n",
      "train\n",
      "test\n",
      "0.27\n",
      "('SIMULATION', 11, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 37.5, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 15. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.31\n",
      "('SIMULATION', 12, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 60.0, 'beta': 100. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 20. * msecond, 'c_intr_out': 15.0})\n",
      "train\n",
      "test\n",
      "0.25\n",
      "('SIMULATION', 13, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 15.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 20. * msecond, 'c_intr_out': 5.0})\n",
      "train\n",
      "test\n",
      "0.26\n",
      "('SIMULATION', 14, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 15.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 30. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.36\n",
      "('SIMULATION', 15, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 30.0, 'beta': 110. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 25. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.24\n",
      "('SIMULATION', 16, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 50.0, 'beta': 140. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 15. * msecond, 'c_intr_out': 17.5})\n",
      "train\n",
      "test\n",
      "0.41\n",
      "('SIMULATION', 17, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 200.0, 'beta': 100. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 30. * msecond, 'c_intr_out': 0.0})\n",
      "train\n",
      "test\n",
      "0.29\n",
      "('SIMULATION', 18, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 150.0, 'beta': 110. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 10. * msecond, 'c_intr_out': 0.0})\n",
      "train\n",
      "test\n",
      "0.36\n",
      "('SIMULATION', 19, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 25.0, 'beta': 80. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 30. * msecond, 'c_intr_out': 25.0})\n",
      "train\n",
      "test\n",
      "0.46\n",
      "('SIMULATION', 20, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 15.0, 'beta': 100. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 20. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    \"i\" is an internal variable of group \"synapses_4\", but also exists in the run namespace with the value 15. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SIMULATION', 21, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 50.0, 'beta': 80. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 25. * msecond, 'c_intr_out': 12.5})\n",
      "train\n",
      "test\n",
      "0.47\n",
      "('SIMULATION', 22, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 150.0, 'beta': 110. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 15. * msecond, 'c_intr_out': 1.0})\n",
      "train\n",
      "test\n",
      "0.62\n",
      "('SIMULATION', 23, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 5.0, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 20. * msecond, 'c_intr_out': 1.0})\n",
      "train\n",
      "test\n",
      "0.58\n",
      "('SIMULATION', 24, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 2.5, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 25. * msecond, 'c_intr_out': 0.10000000000000001})\n",
      "train\n",
      "test\n",
      "0.23\n",
      "('SIMULATION', 25, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 45.0, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 20. * msecond, 'c_intr_out': 15.0})\n",
      "train\n",
      "test\n",
      "0.39\n",
      "('SIMULATION', 26, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 20.0, 'beta': 100. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 30. * msecond, 'c_intr_out': 3.5})\n",
      "train\n",
      "test\n",
      "0.5\n",
      "('SIMULATION', 27, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 100.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 15. * msecond, 'c_intr_out': 17.5})\n",
      "train\n",
      "test\n",
      "0.26\n",
      "('SIMULATION', 28, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 112.5, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 20. * msecond, 'c_intr_out': 0.75})\n",
      "train\n",
      "test\n",
      "0.61\n",
      "('SIMULATION', 29, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 100.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 10. * msecond, 'c_intr_out': 12.5})\n",
      "train\n",
      "test\n",
      "0.54\n",
      "('SIMULATION', 30, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 15.0, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 15. * msecond, 'c_intr_out': 22.5})\n",
      "train\n",
      "test\n",
      "0.23\n",
      "('SIMULATION', 31, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 45.0, 'beta': 80. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 20. * msecond, 'c_intr_out': 3.0})\n",
      "train\n",
      "test\n",
      "0.67\n",
      "('SIMULATION', 32, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 18.75, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 15. * msecond, 'c_intr_out': 0.0})\n",
      "train\n",
      "test\n",
      "0.4\n",
      "('SIMULATION', 33, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 50.0, 'beta': 140. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 15. * msecond, 'c_intr_out': 5.0})\n",
      "train\n",
      "test\n",
      "0.25\n",
      "('SIMULATION', 34, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 30.0, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 20. * msecond, 'c_intr_out': 0.29999999999999999})\n",
      "train\n",
      "test\n",
      "0.6\n",
      "('SIMULATION', 35, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 10.0, 'beta': 110. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 20. * msecond, 'c_intr_out': 0.10000000000000001})\n",
      "train\n",
      "test\n",
      "0.53\n",
      "('SIMULATION', 36, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 7.5, 'beta': 100. * msecond, 'lr': 0.1, 'c_inp': 0.29999999999999999, 'alpha': 20. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.38\n",
      "('SIMULATION', 37, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 2.5, 'beta': 110. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 10. * msecond, 'c_intr_out': 3.5})\n",
      "train\n",
      "test\n",
      "0.11\n",
      "('SIMULATION', 38, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 50.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 15. * msecond, 'c_intr_out': 10.0})\n",
      "train\n",
      "test\n",
      "0.47\n",
      "('SIMULATION', 39, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 25.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 30. * msecond, 'c_intr_out': 25.0})\n",
      "train\n",
      "test\n",
      "0.29\n",
      "('SIMULATION', 40, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 75.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 30. * msecond, 'c_intr_out': 0.5})\n",
      "train\n",
      "test\n",
      "0.95\n",
      "('SIMULATION', 41, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 10.0, 'beta': 90. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 30. * msecond, 'c_intr_out': 7.5})\n",
      "train\n",
      "test\n",
      "0.28\n",
      "('SIMULATION', 42, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 100.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 15. * msecond, 'c_intr_out': 10.0})\n",
      "train\n",
      "test\n",
      "0.55\n",
      "('SIMULATION', 43, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 150.0, 'beta': 80. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 25. * msecond, 'c_intr_out': 0.0})\n",
      "train\n",
      "test\n",
      "0.27\n",
      "('SIMULATION', 44, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 100.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 15. * msecond, 'c_intr_out': 5.0})\n",
      "train\n",
      "test\n",
      "0.47\n",
      "('SIMULATION', 45, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 75.0, 'beta': 120. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 15. * msecond, 'c_intr_out': 56.25})\n",
      "train\n",
      "test\n",
      "0.25\n",
      "('SIMULATION', 46, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 18.75, 'beta': 100. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 25. * msecond, 'c_intr_out': 18.75})\n",
      "train\n",
      "test\n",
      "0.41\n",
      "('SIMULATION', 47, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 75.0, 'beta': 140. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 20. * msecond, 'c_intr_out': 17.5})\n",
      "train\n",
      "test\n",
      "0.26\n",
      "('SIMULATION', 48, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 75.0, 'beta': 80. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 15. * msecond, 'c_intr_out': 0.5})\n",
      "train\n",
      "test\n",
      "0.52\n",
      "('SIMULATION', 49, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 18.75, 'beta': 150. * msecond, 'lr': 0.1, 'c_inp': 0.75, 'alpha': 25. * msecond, 'c_intr_out': 0.75})\n",
      "train\n",
      "test\n",
      "0.55\n",
      "('SIMULATION', 50, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 2.5, 'beta': 140. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 10. * msecond, 'c_intr_out': 0.10000000000000001})\n",
      "train\n",
      "test\n",
      "0.28\n",
      "('SIMULATION', 51, {'c_diff': 1.0, 'lr_intr': 0.1, 'Teacher_amplitude': 100.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 30. * msecond, 'c_intr_out': 0.5})\n",
      "train\n",
      "test\n",
      "0.64\n",
      "('SIMULATION', 52, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 20.0, 'beta': 110. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 15. * msecond, 'c_intr_out': 5.0})\n",
      "train\n",
      "test\n",
      "0.29\n",
      "('SIMULATION', 53, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 5.0, 'beta': 150. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 20. * msecond, 'c_intr_out': 5.0})\n",
      "train\n",
      "test\n",
      "0.24\n",
      "('SIMULATION', 54, {'c_diff': 1.1000000000000001, 'lr_intr': 0.1, 'Teacher_amplitude': 150.0, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 10. * msecond, 'c_intr_out': 0.0})\n",
      "train\n",
      "test\n",
      "0.21\n",
      "('SIMULATION', 55, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 12.5, 'beta': 130. * msecond, 'lr': 0.1, 'c_inp': 0.5, 'alpha': 15. * msecond, 'c_intr_out': 12.5})\n",
      "train\n",
      "test\n",
      "0.38\n",
      "('SIMULATION', 56, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 2.5, 'beta': 150. * msecond, 'lr': 0.1, 'c_inp': 0.10000000000000001, 'alpha': 20. * msecond, 'c_intr_out': 0.10000000000000001})\n",
      "train\n",
      "test\n",
      "0.26\n",
      "('SIMULATION', 57, {'c_diff': 1.2, 'lr_intr': 0.1, 'Teacher_amplitude': 100.0, 'beta': 140. * msecond, 'lr': 0.1, 'c_inp': 1.0, 'alpha': 15. * msecond, 'c_intr_out': 75.0})\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "inits=None\n",
    "monitor = {} # dict with names of objects to record and their parameters\n",
    "monitor['H'] = {'variables':['a'], 'dt':1*ms, 'record':True}\n",
    "#monitor['H'] = {'variables':['I_inp','I_intr','I','a'], 'dt':1*ms, 'record':True}\n",
    "#monitor['Intrinsic_output_weights'] ={'variables':['w'], 'dt':50*ms, 'record':True}\n",
    "#monitor['Input_weights'] ={'variables':['w'], 'dt':50*ms, 'record':True}\n",
    "classes = int(len(set(y)))\n",
    "n_input = int(X.shape[1])\n",
    "\n",
    "\n",
    "#params_to_optimize = [alpha, beta, lr, lr_intr, c_inp, c_intr_out, c_diff, Teacher_amplitude]\n",
    "\n",
    "#NN = Perceptron(X, y, params_to_optimize, inits=inits, monitor=monitor, mod=True, high_verbosity=True)\n",
    "for j in np.arange(100):\n",
    "    \n",
    "    alpha = np.random.choice([10, 15, 20, 25, 30])*ms\n",
    "    beta = np.random.choice([80, 90, 100, 110, 120, 130, 140, 150])*ms\n",
    "    lr = 0.1\n",
    "    lr_intr = 0.1\n",
    "    c_inp = np.random.choice([0.1, 0.3, 0.5, 0.75, 1.0])\n",
    "    c_intr_out = c_inp*np.random.choice([0, 1, 10, 25, 35, 50, 75])\n",
    "    c_diff = np.random.choice([1.0, 1.1, 1.2])\n",
    "    Teacher_amplitude = c_inp*np.random.choice([25,50,100,150,200])\n",
    "\n",
    "    params_to_optimize = {}\n",
    "    params_to_optimize['alpha'] = alpha\n",
    "    params_to_optimize['beta'] = beta\n",
    "    params_to_optimize['lr'] = lr\n",
    "    params_to_optimize['lr_intr'] = lr_intr\n",
    "    params_to_optimize['c_inp'] = c_inp\n",
    "    params_to_optimize['c_intr_out'] = c_intr_out\n",
    "    params_to_optimize['c_diff'] = c_diff\n",
    "    params_to_optimize['Teacher_amplitude'] = Teacher_amplitude\n",
    "\n",
    "    NN = Perceptron(X, y, params_to_optimize, inits=inits, monitor=monitor, mod=True, high_verbosity=False)\n",
    "    \n",
    "    print ('SIMULATION', j, params_to_optimize)\n",
    "    NN.mod = True\n",
    "    print('train')\n",
    "    NN.run(80000*ms) #400\n",
    "    NN.mod = False\n",
    "    print('test')\n",
    "    NN.run(20000*ms) #100\n",
    "    tries = [accuracy_score(NN.shown_labels[-100:], NN.predictions[-100:]), params_to_optimize]\n",
    "    accuracies.append(tries)\n",
    "    #accuracies.append(accuracy_score(NN.shown_labels[-100:], NN.predictions[-100:]))\n",
    "    print (accuracies[-1][0])\n",
    "\n",
    "accs = []\n",
    "for acc in accuracies:\n",
    "    accs.append(acc[0])\n",
    "\n",
    "\n",
    "print (accuracies[np.argmax(accs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (accuracies[np.argmax(accs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.9 SIMULATION 8 {'alpha': 25. * msecond, 'beta': 150. * msecond, 'lr': 0.1, 'lr_intr': 0.1, 'c_inp': 0.10000000000000001, 'c_intr_out': 1.0, 'c_diff': 1.1000000000000001, 'Teacher_amplitude': 15.0}\n",
    "#0.88 SIMULATION 16 {'alpha': 30. * msecond, 'beta': 110. * msecond, 'lr': 0.1, 'lr_intr': 0.1, 'c_inp': 1.0, 'c_intr_out': 1.0, 'c_diff': 1.2, 'Teacher_amplitude': 150.0}\n",
    "#0.87 SIMULATION 12 {'alpha': 25. * msecond, 'beta': 90. * msecond, 'lr': 0.1, 'lr_intr': 0.1, 'c_inp': 0.5, 'c_intr_out': 5.0, 'c_diff': 1.0, 'Teacher_amplitude': 12.5}\n",
    "#0.82 SIMULATION 14 {'alpha': 30. * msecond, 'beta': 130. * msecond, 'lr': 0.1, 'lr_intr': 0.1, 'c_inp': 0.10000000000000001, 'c_intr_out': 0.10000000000000001, 'c_diff': 1.2, 'Teacher_amplitude': 10.0}\n",
    "#0.75 SIMULATION 5 {'alpha': 25. * msecond, 'beta': 120. * msecond, 'lr': 0.1, 'lr_intr': 0.1, 'c_inp': 0.29999999999999999, 'c_intr_out': 0.0, 'c_diff': 1.2, 'Teacher_amplitude': 30.0}\n",
    "#0.75 {'alpha': 25. * msecond, 'beta': 110. * msecond, 'lr': 0.1, 'lr_intr': 0.1, 'c_inp': 0.3, 'c_intr_out': 3.0, 'c_diff': 1.1, 'Teacher_amplitude': 25.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.plot_clocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.imshow_forward_weights('H', 5, 2, 5, 5)\n",
    "NN.imshow_intrinsic_weights('H', 5, 2, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NN.plot_H(monitor['H']['variables'], np.arange(55000,60000), np.arange(len(NN.H)))\n",
    "NN.plot_H(monitor['H']['variables'], NN.StateMonitorH.t/ms, np.arange(len(NN.H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.plot_weights('Input_weights', NN.SynapseMonitorInput.t/ms, np.arange(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.plot_weights('Intrinsic_output_weights', NN.SynapseMonitorIntrinsicOutput.t/ms, np.arange(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.save_weights('./Stupid_digits_saved_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
